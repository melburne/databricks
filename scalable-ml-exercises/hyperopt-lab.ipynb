{"cells":[{"cell_type":"markdown","source":["d-sandbox\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n</div>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"351ab342-1bb7-480d-94ad-0467e2af3ea0"}}},{"cell_type":"markdown","source":["# Hyperopt Lab\n\nThe [Hyperopt library](https://github.com/hyperopt/hyperopt) allows for parallel hyperparameter tuning using either random search or Tree of Parzen Estimators (TPE). With MLflow, we can record the hyperparameters and corresponding metrics for each hyperparameter combination. You can read more on [SparkTrials w/ Hyperopt](https://github.com/hyperopt/hyperopt/blob/master/docs/templates/scaleout/spark.md).\n\n## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) In this lesson you:<br>\n- Learn how to distribute tuning tasks when training a single-node machine learning model by using `SparkTrials` class, rather than the default `Trials` class. \n\n> SparkTrials fits and evaluates each model on one Spark executor, allowing massive scale-out for tuning. To use SparkTrials with Hyperopt, simply pass the SparkTrials object to Hyperopt's fmin() function."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c146c982-cdaa-4485-b794-000ef72a89d6"}}},{"cell_type":"code","source":["pip install hyperopt mlflow"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5cd64937-de4a-44a9-9c87-ceb6e9f0f268"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Python interpreter will be restarted.\nRequirement already satisfied: hyperopt in /local_disk0/.ephemeral_nfs/envs/pythonEnv-f889967b-3eb7-4568-9e7b-66c9eb789547/lib/python3.8/site-packages (0.2.5)\nCollecting mlflow\n  Downloading mlflow-1.18.0-py3-none-any.whl (14.2 MB)\nRequirement already satisfied: six in /databricks/python3/lib/python3.8/site-packages (from hyperopt) (1.15.0)\nRequirement already satisfied: cloudpickle in /local_disk0/.ephemeral_nfs/envs/pythonEnv-f889967b-3eb7-4568-9e7b-66c9eb789547/lib/python3.8/site-packages (from hyperopt) (1.6.0)\nRequirement already satisfied: scipy in /databricks/python3/lib/python3.8/site-packages (from hyperopt) (1.5.2)\nRequirement already satisfied: tqdm in /local_disk0/.ephemeral_nfs/envs/pythonEnv-f889967b-3eb7-4568-9e7b-66c9eb789547/lib/python3.8/site-packages (from hyperopt) (4.61.2)\nRequirement already satisfied: numpy in /databricks/python3/lib/python3.8/site-packages (from hyperopt) (1.19.2)\nRequirement already satisfied: networkx&gt;=2.2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-f889967b-3eb7-4568-9e7b-66c9eb789547/lib/python3.8/site-packages (from hyperopt) (2.5.1)\nRequirement already satisfied: future in /local_disk0/.ephemeral_nfs/envs/pythonEnv-f889967b-3eb7-4568-9e7b-66c9eb789547/lib/python3.8/site-packages (from hyperopt) (0.18.2)\nCollecting pyyaml&gt;=5.1\n  Downloading PyYAML-5.4.1-cp38-cp38-manylinux1_x86_64.whl (662 kB)\nCollecting prometheus-flask-exporter\n  Downloading prometheus_flask_exporter-0.18.2.tar.gz (22 kB)\nCollecting alembic&lt;=1.4.1\n  Downloading alembic-1.4.1.tar.gz (1.1 MB)\nCollecting databricks-cli&gt;=0.8.7\n  Downloading databricks-cli-0.14.3.tar.gz (54 kB)\nRequirement already satisfied: protobuf&gt;=3.7.0 in /databricks/python3/lib/python3.8/site-packages (from mlflow) (3.17.0)\nRequirement already satisfied: pytz in /databricks/python3/lib/python3.8/site-packages (from mlflow) (2020.5)\nCollecting click&gt;=7.0\n  Downloading click-8.0.1-py3-none-any.whl (97 kB)\nRequirement already satisfied: requests&gt;=2.17.3 in /databricks/python3/lib/python3.8/site-packages (from mlflow) (2.24.0)\nCollecting docker&gt;=4.0.0\n  Downloading docker-5.0.0-py2.py3-none-any.whl (146 kB)\nCollecting querystring-parser\n  Downloading querystring_parser-1.2.4-py2.py3-none-any.whl (7.9 kB)\nRequirement already satisfied: pandas in /databricks/python3/lib/python3.8/site-packages (from mlflow) (1.1.5)\nRequirement already satisfied: entrypoints in /databricks/python3/lib/python3.8/site-packages (from mlflow) (0.3)\nCollecting packaging\n  Downloading packaging-21.0-py3-none-any.whl (40 kB)\nCollecting Flask\n  Downloading Flask-2.0.1-py3-none-any.whl (94 kB)\nCollecting sqlalchemy\n  Downloading SQLAlchemy-1.4.20-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\nCollecting gitpython&gt;=2.1.0\n  Downloading GitPython-3.1.18-py3-none-any.whl (170 kB)\nCollecting sqlparse&gt;=0.3.1\n  Downloading sqlparse-0.4.1-py3-none-any.whl (42 kB)\nCollecting gunicorn; platform_system != &#34;Windows&#34;\n  Downloading gunicorn-20.1.0-py3-none-any.whl (79 kB)\nRequirement already satisfied: decorator&lt;5,&gt;=4.3 in /databricks/python3/lib/python3.8/site-packages (from networkx&gt;=2.2-&gt;hyperopt) (4.4.2)\nCollecting prometheus_client\n  Downloading prometheus_client-0.11.0-py2.py3-none-any.whl (56 kB)\nCollecting Mako\n  Downloading Mako-1.1.4-py2.py3-none-any.whl (75 kB)\nCollecting python-editor&gt;=0.3\n  Downloading python_editor-1.0.4-py3-none-any.whl (4.9 kB)\nRequirement already satisfied: python-dateutil in /databricks/python3/lib/python3.8/site-packages (from alembic&lt;=1.4.1-&gt;mlflow) (2.8.1)\nCollecting tabulate&gt;=0.7.7\n  Downloading tabulate-0.8.9-py3-none-any.whl (25 kB)\nRequirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /databricks/python3/lib/python3.8/site-packages (from requests&gt;=2.17.3-&gt;mlflow) (3.0.4)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 in /databricks/python3/lib/python3.8/site-packages (from requests&gt;=2.17.3-&gt;mlflow) (1.25.11)\nRequirement already satisfied: idna&lt;3,&gt;=2.5 in /databricks/python3/lib/python3.8/site-packages (from requests&gt;=2.17.3-&gt;mlflow) (2.10)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /databricks/python3/lib/python3.8/site-packages (from requests&gt;=2.17.3-&gt;mlflow) (2020.12.5)\nCollecting websocket-client&gt;=0.32.0\n  Downloading websocket_client-1.1.0-py2.py3-none-any.whl (68 kB)\nRequirement already satisfied: pyparsing&gt;=2.0.2 in /databricks/python3/lib/python3.8/site-packages (from packaging-&gt;mlflow) (2.4.7)\nCollecting Werkzeug&gt;=2.0\n  Downloading Werkzeug-2.0.1-py3-none-any.whl (288 kB)\nCollecting itsdangerous&gt;=2.0\n  Downloading itsdangerous-2.0.1-py3-none-any.whl (18 kB)\nCollecting Jinja2&gt;=3.0\n  Downloading Jinja2-3.0.1-py3-none-any.whl (133 kB)\nCollecting greenlet!=0.4.17; python_version &gt;= &#34;3&#34;\n  Downloading greenlet-1.1.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (164 kB)\nCollecting gitdb&lt;5,&gt;=4.0.1\n  Downloading gitdb-4.0.7-py3-none-any.whl (63 kB)\nRequirement already satisfied: setuptools&gt;=3.0 in /databricks/python3/lib/python3.8/site-packages (from gunicorn; platform_system != &#34;Windows&#34;-&gt;mlflow) (50.3.1)\nCollecting MarkupSafe&gt;=0.9.2\n  Downloading MarkupSafe-2.0.1-cp38-cp38-manylinux2010_x86_64.whl (30 kB)\nCollecting smmap&lt;5,&gt;=3.0.1\n  Downloading smmap-4.0.0-py2.py3-none-any.whl (24 kB)\nBuilding wheels for collected packages: prometheus-flask-exporter, alembic, databricks-cli\n  Building wheel for prometheus-flask-exporter (setup.py): started\n  Building wheel for prometheus-flask-exporter (setup.py): finished with status &#39;done&#39;\n  Created wheel for prometheus-flask-exporter: filename=prometheus_flask_exporter-0.18.2-py3-none-any.whl size=17398 sha256=5560e26d40ea4acd39a819124d3621450646eea6c67650f02e898fe7b21deb7d\n  Stored in directory: /root/.cache/pip/wheels/69/6f/b4/2087abb1172ae32c58e366dc09746de46a72b0e9fb2c022920\n  Building wheel for alembic (setup.py): started\n  Building wheel for alembic (setup.py): finished with status &#39;done&#39;\n  Created wheel for alembic: filename=alembic-1.4.1-py2.py3-none-any.whl size=158155 sha256=c792ddfb2b1b4219e68dc0481bdeeb50c01e863deea8addab413ad48609c2456\n  Stored in directory: /root/.cache/pip/wheels/9d/de/6d/ca8d461ec29e010b1267d7353d0b058819770f7680bb9360e4\n  Building wheel for databricks-cli (setup.py): started\n  Building wheel for databricks-cli (setup.py): finished with status &#39;done&#39;\n  Created wheel for databricks-cli: filename=databricks_cli-0.14.3-py3-none-any.whl size=100556 sha256=6be3fbb2bb6a7029d4a376cc88e48eae817cc50f3f9340eba4ab1770dc55e2f3\n  Stored in directory: /root/.cache/pip/wheels/e9/f3/dc/eeff77dbc147629fa716741fc216520abbc0e15ce4b876706f\nSuccessfully built prometheus-flask-exporter alembic databricks-cli\nInstalling collected packages: pyyaml, prometheus-client, click, Werkzeug, itsdangerous, MarkupSafe, Jinja2, Flask, prometheus-flask-exporter, greenlet, sqlalchemy, Mako, python-editor, alembic, tabulate, databricks-cli, websocket-client, docker, querystring-parser, packaging, smmap, gitdb, gitpython, sqlparse, gunicorn, mlflow\nSuccessfully installed Flask-2.0.1 Jinja2-3.0.1 Mako-1.1.4 MarkupSafe-2.0.1 Werkzeug-2.0.1 alembic-1.4.1 click-8.0.1 databricks-cli-0.14.3 docker-5.0.0 gitdb-4.0.7 gitpython-3.1.18 greenlet-1.1.0 gunicorn-20.1.0 itsdangerous-2.0.1 mlflow-1.18.0 packaging-21.0 prometheus-client-0.11.0 prometheus-flask-exporter-0.18.2 python-editor-1.0.4 pyyaml-5.4.1 querystring-parser-1.2.4 smmap-4.0.0 sqlalchemy-1.4.20 sqlparse-0.4.1 tabulate-0.8.9 websocket-client-1.1.0\nWARNING: You are using pip version 20.2.4; however, version 21.1.3 is available.\nYou should consider upgrading via the &#39;/local_disk0/.ephemeral_nfs/envs/pythonEnv-f889967b-3eb7-4568-9e7b-66c9eb789547/bin/python -m pip install --upgrade pip&#39; command.\nPython interpreter will be restarted.\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Python interpreter will be restarted.\nRequirement already satisfied: hyperopt in /local_disk0/.ephemeral_nfs/envs/pythonEnv-f889967b-3eb7-4568-9e7b-66c9eb789547/lib/python3.8/site-packages (0.2.5)\nCollecting mlflow\n  Downloading mlflow-1.18.0-py3-none-any.whl (14.2 MB)\nRequirement already satisfied: six in /databricks/python3/lib/python3.8/site-packages (from hyperopt) (1.15.0)\nRequirement already satisfied: cloudpickle in /local_disk0/.ephemeral_nfs/envs/pythonEnv-f889967b-3eb7-4568-9e7b-66c9eb789547/lib/python3.8/site-packages (from hyperopt) (1.6.0)\nRequirement already satisfied: scipy in /databricks/python3/lib/python3.8/site-packages (from hyperopt) (1.5.2)\nRequirement already satisfied: tqdm in /local_disk0/.ephemeral_nfs/envs/pythonEnv-f889967b-3eb7-4568-9e7b-66c9eb789547/lib/python3.8/site-packages (from hyperopt) (4.61.2)\nRequirement already satisfied: numpy in /databricks/python3/lib/python3.8/site-packages (from hyperopt) (1.19.2)\nRequirement already satisfied: networkx&gt;=2.2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-f889967b-3eb7-4568-9e7b-66c9eb789547/lib/python3.8/site-packages (from hyperopt) (2.5.1)\nRequirement already satisfied: future in /local_disk0/.ephemeral_nfs/envs/pythonEnv-f889967b-3eb7-4568-9e7b-66c9eb789547/lib/python3.8/site-packages (from hyperopt) (0.18.2)\nCollecting pyyaml&gt;=5.1\n  Downloading PyYAML-5.4.1-cp38-cp38-manylinux1_x86_64.whl (662 kB)\nCollecting prometheus-flask-exporter\n  Downloading prometheus_flask_exporter-0.18.2.tar.gz (22 kB)\nCollecting alembic&lt;=1.4.1\n  Downloading alembic-1.4.1.tar.gz (1.1 MB)\nCollecting databricks-cli&gt;=0.8.7\n  Downloading databricks-cli-0.14.3.tar.gz (54 kB)\nRequirement already satisfied: protobuf&gt;=3.7.0 in /databricks/python3/lib/python3.8/site-packages (from mlflow) (3.17.0)\nRequirement already satisfied: pytz in /databricks/python3/lib/python3.8/site-packages (from mlflow) (2020.5)\nCollecting click&gt;=7.0\n  Downloading click-8.0.1-py3-none-any.whl (97 kB)\nRequirement already satisfied: requests&gt;=2.17.3 in /databricks/python3/lib/python3.8/site-packages (from mlflow) (2.24.0)\nCollecting docker&gt;=4.0.0\n  Downloading docker-5.0.0-py2.py3-none-any.whl (146 kB)\nCollecting querystring-parser\n  Downloading querystring_parser-1.2.4-py2.py3-none-any.whl (7.9 kB)\nRequirement already satisfied: pandas in /databricks/python3/lib/python3.8/site-packages (from mlflow) (1.1.5)\nRequirement already satisfied: entrypoints in /databricks/python3/lib/python3.8/site-packages (from mlflow) (0.3)\nCollecting packaging\n  Downloading packaging-21.0-py3-none-any.whl (40 kB)\nCollecting Flask\n  Downloading Flask-2.0.1-py3-none-any.whl (94 kB)\nCollecting sqlalchemy\n  Downloading SQLAlchemy-1.4.20-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\nCollecting gitpython&gt;=2.1.0\n  Downloading GitPython-3.1.18-py3-none-any.whl (170 kB)\nCollecting sqlparse&gt;=0.3.1\n  Downloading sqlparse-0.4.1-py3-none-any.whl (42 kB)\nCollecting gunicorn; platform_system != &#34;Windows&#34;\n  Downloading gunicorn-20.1.0-py3-none-any.whl (79 kB)\nRequirement already satisfied: decorator&lt;5,&gt;=4.3 in /databricks/python3/lib/python3.8/site-packages (from networkx&gt;=2.2-&gt;hyperopt) (4.4.2)\nCollecting prometheus_client\n  Downloading prometheus_client-0.11.0-py2.py3-none-any.whl (56 kB)\nCollecting Mako\n  Downloading Mako-1.1.4-py2.py3-none-any.whl (75 kB)\nCollecting python-editor&gt;=0.3\n  Downloading python_editor-1.0.4-py3-none-any.whl (4.9 kB)\nRequirement already satisfied: python-dateutil in /databricks/python3/lib/python3.8/site-packages (from alembic&lt;=1.4.1-&gt;mlflow) (2.8.1)\nCollecting tabulate&gt;=0.7.7\n  Downloading tabulate-0.8.9-py3-none-any.whl (25 kB)\nRequirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /databricks/python3/lib/python3.8/site-packages (from requests&gt;=2.17.3-&gt;mlflow) (3.0.4)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 in /databricks/python3/lib/python3.8/site-packages (from requests&gt;=2.17.3-&gt;mlflow) (1.25.11)\nRequirement already satisfied: idna&lt;3,&gt;=2.5 in /databricks/python3/lib/python3.8/site-packages (from requests&gt;=2.17.3-&gt;mlflow) (2.10)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /databricks/python3/lib/python3.8/site-packages (from requests&gt;=2.17.3-&gt;mlflow) (2020.12.5)\nCollecting websocket-client&gt;=0.32.0\n  Downloading websocket_client-1.1.0-py2.py3-none-any.whl (68 kB)\nRequirement already satisfied: pyparsing&gt;=2.0.2 in /databricks/python3/lib/python3.8/site-packages (from packaging-&gt;mlflow) (2.4.7)\nCollecting Werkzeug&gt;=2.0\n  Downloading Werkzeug-2.0.1-py3-none-any.whl (288 kB)\nCollecting itsdangerous&gt;=2.0\n  Downloading itsdangerous-2.0.1-py3-none-any.whl (18 kB)\nCollecting Jinja2&gt;=3.0\n  Downloading Jinja2-3.0.1-py3-none-any.whl (133 kB)\nCollecting greenlet!=0.4.17; python_version &gt;= &#34;3&#34;\n  Downloading greenlet-1.1.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (164 kB)\nCollecting gitdb&lt;5,&gt;=4.0.1\n  Downloading gitdb-4.0.7-py3-none-any.whl (63 kB)\nRequirement already satisfied: setuptools&gt;=3.0 in /databricks/python3/lib/python3.8/site-packages (from gunicorn; platform_system != &#34;Windows&#34;-&gt;mlflow) (50.3.1)\nCollecting MarkupSafe&gt;=0.9.2\n  Downloading MarkupSafe-2.0.1-cp38-cp38-manylinux2010_x86_64.whl (30 kB)\nCollecting smmap&lt;5,&gt;=3.0.1\n  Downloading smmap-4.0.0-py2.py3-none-any.whl (24 kB)\nBuilding wheels for collected packages: prometheus-flask-exporter, alembic, databricks-cli\n  Building wheel for prometheus-flask-exporter (setup.py): started\n  Building wheel for prometheus-flask-exporter (setup.py): finished with status &#39;done&#39;\n  Created wheel for prometheus-flask-exporter: filename=prometheus_flask_exporter-0.18.2-py3-none-any.whl size=17398 sha256=5560e26d40ea4acd39a819124d3621450646eea6c67650f02e898fe7b21deb7d\n  Stored in directory: /root/.cache/pip/wheels/69/6f/b4/2087abb1172ae32c58e366dc09746de46a72b0e9fb2c022920\n  Building wheel for alembic (setup.py): started\n  Building wheel for alembic (setup.py): finished with status &#39;done&#39;\n  Created wheel for alembic: filename=alembic-1.4.1-py2.py3-none-any.whl size=158155 sha256=c792ddfb2b1b4219e68dc0481bdeeb50c01e863deea8addab413ad48609c2456\n  Stored in directory: /root/.cache/pip/wheels/9d/de/6d/ca8d461ec29e010b1267d7353d0b058819770f7680bb9360e4\n  Building wheel for databricks-cli (setup.py): started\n  Building wheel for databricks-cli (setup.py): finished with status &#39;done&#39;\n  Created wheel for databricks-cli: filename=databricks_cli-0.14.3-py3-none-any.whl size=100556 sha256=6be3fbb2bb6a7029d4a376cc88e48eae817cc50f3f9340eba4ab1770dc55e2f3\n  Stored in directory: /root/.cache/pip/wheels/e9/f3/dc/eeff77dbc147629fa716741fc216520abbc0e15ce4b876706f\nSuccessfully built prometheus-flask-exporter alembic databricks-cli\nInstalling collected packages: pyyaml, prometheus-client, click, Werkzeug, itsdangerous, MarkupSafe, Jinja2, Flask, prometheus-flask-exporter, greenlet, sqlalchemy, Mako, python-editor, alembic, tabulate, databricks-cli, websocket-client, docker, querystring-parser, packaging, smmap, gitdb, gitpython, sqlparse, gunicorn, mlflow\nSuccessfully installed Flask-2.0.1 Jinja2-3.0.1 Mako-1.1.4 MarkupSafe-2.0.1 Werkzeug-2.0.1 alembic-1.4.1 click-8.0.1 databricks-cli-0.14.3 docker-5.0.0 gitdb-4.0.7 gitpython-3.1.18 greenlet-1.1.0 gunicorn-20.1.0 itsdangerous-2.0.1 mlflow-1.18.0 packaging-21.0 prometheus-client-0.11.0 prometheus-flask-exporter-0.18.2 python-editor-1.0.4 pyyaml-5.4.1 querystring-parser-1.2.4 smmap-4.0.0 sqlalchemy-1.4.20 sqlparse-0.4.1 tabulate-0.8.9 websocket-client-1.1.0\nWARNING: You are using pip version 20.2.4; however, version 21.1.3 is available.\nYou should consider upgrading via the &#39;/local_disk0/.ephemeral_nfs/envs/pythonEnv-f889967b-3eb7-4568-9e7b-66c9eb789547/bin/python -m pip install --upgrade pip&#39; command.\nPython interpreter will be restarted.\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["%run \"../Includes/Classroom-Setup\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5e4b801a-359c-4fb4-a456-0b1d0e23760e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Initialized classroom variables & functions...","textData":null,"removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"htmlSandbox","arguments":{}}},"output_type":"display_data","data":{"text/html":["Initialized classroom variables & functions..."]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Datasets are already mounted to <b>/mnt/training</b> from <b>s3a://databricks-corp-training/common</b>","textData":null,"removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"htmlSandbox","arguments":{}}},"output_type":"display_data","data":{"text/html":["Datasets are already mounted to <b>/mnt/training</b> from <b>s3a://databricks-corp-training/common</b>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"\n<div>Declared various utility methods:</div>\n<li>Declared <b style=\"color:green\">untilStreamIsReady(<i>name:String</i>)</b> to control workflow</li>\n<br/>\n<div>All done!</div>\n","textData":null,"removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"htmlSandbox","arguments":{}}},"output_type":"display_data","data":{"text/html":["\n<div>Declared various utility methods:</div>\n<li>Declared <b style=\"color:green\">untilStreamIsReady(<i>name:String</i>)</b> to control workflow</li>\n<br/>\n<div>All done!</div>\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["Read in a cleaned version of the Airbnb dataset with just numeric features."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a52c68d5-bd3b-4317-ad36-a13af72cc398"}}},{"cell_type":"code","source":["# did not work\n# dbutils.fs.cp(\"dbfs:/mnt/training/airbnb/sf-listings/airbnb-cleaned-mlflow.csv\", \"/\", True)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"13284a49-c21a-4cee-8a85-99ed749f15f7"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[62]: True</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[62]: True</div>"]}}],"execution_count":0},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\nimport pandas as pd\n\n# does not work on DB community edition\n# df = pd.read_csv(\"/dbfs/mnt/training/airbnb/sf-listings/airbnb-cleaned-mlflow.csv\").drop([\"zipcode\"], axis=1)\n\n# workaround is to read using spark and then convert to pd dataframe\nsparkDf = spark.read.option(\"header\", \"true\").csv(\"dbfs:/mnt/training/airbnb/sf-listings/airbnb-cleaned-mlflow.csv\").drop(\"zipcode\")\ndf = sparkDf.select(\"*\").toPandas()\n\n# split 80/20 train-test\nX_train, X_test, y_train, y_test = train_test_split(df.drop([\"price\"], axis=1),\n                                                    df[[\"price\"]].values.ravel(),\n                                                    test_size = 0.2,\n                                                    random_state = 42)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c90d1d27-0993-42e1-bb8e-c02ade167624"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Now we need to define an `objective_function` where you evaluate the [random forest's](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html) predictions using R2.\n\nIn the code below, compute the `r2` and return it along with `STATUS_OK` (remember we are trying to maximize R2, so we need to return it as a negative value)."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"82d0e8b3-8e8b-44ae-bd88-076bd819f65d"}}},{"cell_type":"code","source":["# TODO\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import make_scorer, r2_score\nfrom numpy import mean\nfrom hyperopt import STATUS_OK\n  \ndef objective_function(params):\n\n  # set the hyperparameters that we want to tune:\n  max_depth = params[\"max_depth\"]\n  max_features = params[\"max_features\"]\n\n  regressor = RandomForestRegressor(max_depth=max_depth, max_features=max_features, random_state=42)\n\n  # Evaluate predictions\n  r2 = mean(cross_val_score(regressor, X_train, y_train, cv=3))\n\n  # Note: since we aim to maximize r2, we need to return it as a negative value (\"loss\": -metric)\n  return {\"loss\": -r2, \"status\": STATUS_OK}"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f4cd5d41-422e-4635-bba3-d94ab7bb8d87"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["We need to define a search space for HyperOpt. Let the `max_depth` vary between 2-10, and `max_features` be one of: \"auto\", \"sqrt\", or \"log2\"."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e939c372-69bc-43fd-92f4-155a35bec5df"}}},{"cell_type":"code","source":["# TODO\nfrom hyperopt import hp\n\nsearch_space = {\n  \"max_depth\": hp.randint(\"max_depth\", 2, 10),\n  \"max_features\": hp.choice(\"max_features\", [\"auto\", \"sqrt\", \"log2\"])\n}"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"32fb0528-b0a8-484d-b05f-60f40d31a049"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Instead of using the default `Trials` class, you can leverage the `SparkTrials` class to trigger the distribution of tuning tasks across Spark executors. On Databricks, SparkTrials are automatically logged with MLflow.\n\n`SparkTrials` takes 3 optional arguments, namely `parallelism`, `timeout`, and `spark_session`. You can refer to this [page](http://hyperopt.github.io/hyperopt/scaleout/spark/) to read more.\n\nIn the code below, fill in the `fmin` function."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8996860f-a4a5-4c87-8256-5ab095d56906"}}},{"cell_type":"code","source":["# TODO\nfrom hyperopt import fmin, tpe, STATUS_OK, SparkTrials\nimport mlflow\n\n# Creating a parent run\nwith mlflow.start_run():\n  # the number of models we want to evaluate\n  num_evals = 8\n  # set the number of models to be trained concurrently\n#   spark_trials = SparkTrials(parallelism=2)\n  best_hyperparam = fmin(fn=objective_function,\n                         space=search_space,\n                         algo=tpe.suggest,\n#                          trials=spark_trials,\n                         max_evals=num_evals)\n\n  # get optimal hyperparameter values\n  best_max_depth = best_hyperparam[\"max_depth\"]\n  best_max_features = best_hyperparam[\"max_features\"]\n\n  # train model on entire training data\n  regressor = RandomForestRegressor(max_depth=best_max_depth, max_features=best_max_features, random_state=42)\n  regressor.fit(X_train, y_train)\n\n  # evaluate on holdout/test data\n  r2 = regressor.score(X_test, y_test)\n  \n  # Log param and metric for the final model\n  mlflow.log_param(\"max_depth\", best_max_depth)\n  mlflow.log_param(\"max_features\", best_max_features)\n  mlflow.log_metric(\"loss\", r2)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6c93098e-a0ba-46c1-a8f8-656843dbb4f1"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">\r  0%|          | 0/8 [00:00&lt;?, ?trial/s, best loss=?]\r 12%|█▎        | 1/8 [00:01&lt;00:07,  1.07s/trial, best loss: -0.6566746347967997]\r 25%|██▌       | 2/8 [00:03&lt;00:11,  1.97s/trial, best loss: -0.6566746347967997]\r 38%|███▊      | 3/8 [00:04&lt;00:07,  1.58s/trial, best loss: -0.6566746347967997]\r 50%|█████     | 4/8 [00:05&lt;00:05,  1.40s/trial, best loss: -0.6608776555513131]\r 62%|██████▎   | 5/8 [00:07&lt;00:03,  1.31s/trial, best loss: -0.6608776555513131]\r 75%|███████▌  | 6/8 [00:09&lt;00:03,  1.65s/trial, best loss: -0.6608776555513131]\r 88%|████████▊ | 7/8 [00:11&lt;00:01,  1.97s/trial, best loss: -0.6608776555513131]\r100%|██████████| 8/8 [00:13&lt;00:00,  1.68s/trial, best loss: -0.6608776555513131]\r100%|██████████| 8/8 [00:13&lt;00:00,  1.63s/trial, best loss: -0.6608776555513131]\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">\r  0%|          | 0/8 [00:00&lt;?, ?trial/s, best loss=?]\r 12%|█▎        | 1/8 [00:01&lt;00:07,  1.07s/trial, best loss: -0.6566746347967997]\r 25%|██▌       | 2/8 [00:03&lt;00:11,  1.97s/trial, best loss: -0.6566746347967997]\r 38%|███▊      | 3/8 [00:04&lt;00:07,  1.58s/trial, best loss: -0.6566746347967997]\r 50%|█████     | 4/8 [00:05&lt;00:05,  1.40s/trial, best loss: -0.6608776555513131]\r 62%|██████▎   | 5/8 [00:07&lt;00:03,  1.31s/trial, best loss: -0.6608776555513131]\r 75%|███████▌  | 6/8 [00:09&lt;00:03,  1.65s/trial, best loss: -0.6608776555513131]\r 88%|████████▊ | 7/8 [00:11&lt;00:01,  1.97s/trial, best loss: -0.6608776555513131]\r100%|██████████| 8/8 [00:13&lt;00:00,  1.68s/trial, best loss: -0.6608776555513131]\r100%|██████████| 8/8 [00:13&lt;00:00,  1.63s/trial, best loss: -0.6608776555513131]\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Now you can compare all of the models using the MLflow UI. \n\nTo understand the effect of tuning a hyperparameter:\n\n0. Select the resulting runs and click Compare.\n0. In the Scatter Plot, select a hyperparameter for the X-axis and loss for the Y-axis."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"525c409e-8b48-4f11-a47a-1dd0279451e6"}}},{"cell_type":"markdown","source":["-sandbox\n&copy; 2020 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"http://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"http://help.databricks.com/\">Support</a>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"46e87c7f-3ed6-4ac6-a916-44625b5ce1fe"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"ML 08L - Hyperopt Lab","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2,"experimentId":"1750467221661744"},"language":"python","widgets":{},"notebookOrigID":1750467221661744}},"nbformat":4,"nbformat_minor":0}
